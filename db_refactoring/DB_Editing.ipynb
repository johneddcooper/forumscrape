{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Automaton Forumscrape Project\n",
    "# Data Processing, Sanitation, and Anonymizing\n",
    "\n",
    "This notebook explains the steps used to sanitize the data gathered in the [previous post](https://learningautomaton.ca/2019/03/forum-scrape-project-scrapy-spider/) in this project.\n",
    "\n",
    "The start state is a MongoDB containing thread and post data scraped from the [Fruits and Veggies forum](https://learningautomaton.ca/wp-content/uploads/2019/02/FruitsAndVeggiesForum/Knock%20Knock...%20-%20Fruits%20and%20Veggies.html). \n",
    "\n",
    "You can see a text representation of the database here: [Raw Data Text File](https://github.com/johneddcooper/forumscrape/blob/master/db_refactoring/data_raw)\n",
    "\n",
    "This notebook contains a text copy of the data before and after each stage, so you can see the changes without actually needing a working database. Some steps will work with the included text-data, however, some steps require a working database. These will be clearly indicated. \n",
    "\n",
    "The end state is that the data pulled has been cleaned, stripped of potential [personally identifying information](https://en.wikipedia.org/wiki/Personal_data), and is ready to be processed for data statistics. \n",
    "\n",
    "I delve deeper into the topic of personal information and why it is important to sanitize in the [Ethics Analasys](https://learningautomaton.ca/2019/01/forum-scrape-project-ethics/) for this project.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "For our data and needs, this will be a three-part process:\n",
    "1. Refactor the database to make it more easily accessible, and remove unnecessary fields;\n",
    "2. Replace usernames with unique random identifiers (we don't want to strip them out completely, as we can derive anonymous user statistics;\n",
    "3. Remove unnecessary, potentially identifying, and unknown words and data from post titles and content.\n",
    "\n",
    "We will iteratively write the changes to a copy of the database in each phase.\n",
    "\n",
    "## Tools\n",
    "\n",
    "For database reading and writing we will use PyMongo, to connect to a locally running MongoDB server.  \n",
    "\n",
    "To find data that needs to be stripped, we will use [spaCy](https://spacy.io/), a free, open-source Natural Language Processing library for Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 - Refactoring\n",
    "\n",
    "**Note that for the following steps to work you will need to be running this notebook locally (not on Binder) and have a running MongoDB server.** See the [Forumscrape - Scrapy Spider](https://learningautomaton.ca/2019/03/forum-scrape-project-scrapy-spider/) post for details.\n",
    "\n",
    "The data is currently stored in the following schema:\n",
    "\n",
    "* thread_1\n",
    "    * _id\n",
    "    * title\n",
    "    * url\n",
    "    * posts\n",
    "        * post_1\n",
    "            * user\n",
    "            * title\n",
    "            * content\n",
    "            * datetime\n",
    "        * post_2\n",
    "             <br>...\n",
    "* thread_2\n",
    "    <br>...\n",
    "    \n",
    " \n",
    "This schema was useful for when we were doing the scraping, however, nested documents are not fun to work with. We are only going to be doing simple language statistics on the post content. We can strip out the thread URL and post datetime, and we will promote the posts to be the top-level document. At the end out schema will look like this:\n",
    "* post_1\n",
    "    * thread_title\n",
    "    * user\n",
    "    * content\n",
    "* post_2\n",
    "    <br>...\n",
    "    \n",
    "<br>\n",
    "\n",
    "*Note: a MongoDB \"document\" is an item in the database. It is similar to a dictionary object, in that they have field:value pairs and they can contain other documents (or lists of documents) as values within them.*\n",
    "\n",
    "<br>\n",
    "\n",
    "To accomplish the refactoring, we will make use of MongoDB's [Aggregation Pipelines](https://docs.mongodb.com/manual/core/aggregation-pipeline/).\n",
    "\n",
    "Aggregation Pipelines are extremely powerful, but they can be complex. I won't go into all that is possible, just the steps we need.\n",
    "\n",
    "Pipelines run in sequential stages. The stages we need are:\n",
    "* unwind: make a new top-level document for each sub-document in an embedded array. This will turn every post from sub-documents embedded in threads into their own top-level documents.\n",
    "* project: transform a new document into a new structure. The previous step left the posts as sub-documents in an embedded array or 1. We pull the data out of the embedded sub-document into the top level document.\n",
    "* sort: sort (serves to group) the posts by thread title. Not required, but makes for easier reading and verifying the results.\n",
    "* out: used to write the results of the pipeline to the database, in the given collection. If this stage is omitted, the pipeline call will return a temporary way to see the pipeline transformation without making any changes to the DB. Useful for testing the pipeline before writing, especially for large databases.\n",
    "\n",
    "We construct an array with the parameters we need and pass it to the collection.aggregate(pipeline) function.\n",
    "\n",
    "You can see the end result [here](https://github.com/johneddcooper/forumscrape/blob/master/db_refactoring/data_stg1_refactored)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "# Point the MongoClient at our database. In this case, the MongoDB server is running locally, on port 27017.\n",
    "client = MongoClient('192.168.2.70', 27017)\n",
    "# The DB within the MongoDB server the data is stored on, in my case, in a DB called fruitsandveggies\n",
    "db=client['fruitsandveggies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.command_cursor.CommandCursor at 0x7ae4080>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The 'threads' collection holds my raw data, that was gathered by the Scrapy spider in the previous post\n",
    "collection = db['threads']\n",
    "\n",
    "# making an array with the pipeline parameters, to be passed in using the collection.aggregate() function\n",
    "pipeline =  [ \n",
    "    { \"$unwind\": \"$posts\" }, # Makes a copy of each thread document, for each embedded post document, containing only that post\n",
    "    { \"$project\": \n",
    "         { \"_id\": 0, \n",
    "          \"thread_title\": \"$title\", # Make explicit that the field is the title of the parent thread \n",
    "          \"user\": \"$posts.user\",  # Pull the post user from the sub-document into the document\n",
    "          \"content\": \"$posts.content\" } }, # Pull the post content from the sub-document into the document\n",
    "    { \"$sort\": { \"thread_title\": 1 } }, # Sort the post documents by title\n",
    "    { \"$out\": \"aggregate-posts-out_3\" } # Write to the \"aggregate-posts-out\" collection within the database\n",
    "    ]\n",
    "\n",
    "# Call aggregate on the collection, with our pipeline details. Because these steps are memory intensive, we need to set allowDiskUse to True to allow MongoDB to write temporary files to dist during the operation.\n",
    "# The call will fail otherwise for any non-trivial data set.\n",
    "collection.aggregate(pipeline, allowDiskUse=True)\n",
    "\n",
    "# the call returns a cursor that can be used to navigate the results of the pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 - Replace Username\n",
    "\n",
    "As part of the ethics analysis for this project, we identified a need to hide username: both because they can be used to link the data to users on other sites and because they might contain personally identifying information.\n",
    "\n",
    "The steps are as follows:\n",
    "1. Get all posts from the DB\n",
    "2. Make a dict with usernames as keys\n",
    "3. Assign a unique random number to each user\n",
    "4. Replace all instances of the username with the unique random number in the DB\n",
    "\n",
    "You can see the results [here](https://github.com/johneddcooper/forumscrape/blob/master/db_refactoring/data_stg2_username_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather all posts, get set of usernames, assign a random unique number to each user, replace username with the unique identifier in the database\n",
    "\n",
    "import random\n",
    "import math\n",
    "from math import log10, ceil\n",
    "from random import sample\n",
    "\n",
    "collection = db['aggregate-posts-out_3']\n",
    "post_dict = dict()\n",
    "user_dict = dict()\n",
    "# collection.find() returns a cursor object that can be used to iterate over the results of the search query\n",
    "posts = collection.find({}, {'_id':1, 'user':1})\n",
    "\n",
    "for post in posts: # Call collection.find with two arguments: {} to pull every post, and  {'_id':1, 'user':1} to only get those fields returned\n",
    "    user_dict[post['user']] = None # Make a dict with each user as a key. Ensures we don't assign multiple random ids to the same user\n",
    "\n",
    "num_users = len(user_dict.keys())\n",
    "number_sample=sample(range(1,num_users+1), k=num_users) # Make a range from 1 to n+1, where n is the number of users. Take n samples from our range of n elements (serves to shuffle the numbers)\n",
    "\n",
    "for user in user_dict.keys():\n",
    "    user_dict[user] = number_sample.pop() # Assign each user a number\n",
    "\n",
    "# The our cursor object was exausted, to get a new one with a new find query\n",
    "posts = collection.find({}, {'_id':1, 'user':1})\n",
    "for post in posts:\n",
    "    collection.update_one( # Update each post with the new random user id in place of the username\n",
    "        {'_id':post['_id']},\n",
    "        {'$set':{'user':user_dict[post['user']]}},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 - Strip out Unwanted Words\n",
    "\n",
    "In this stage, we will process the post titles and contents using spaCy.\n",
    "\n",
    "I will go into greater detail into spaCy in the next post, where we will derive some useful statistics and meaning from the data. As a quick introduction: \n",
    "\n",
    "spaCy, once we have loaded one of the language models, will allow us to feed it strings. It will produce a spaCy `Doc` object, which is a series of tokens (a token being defined as a word, space, or punctuation) that were derived from the fed string.\n",
    "\n",
    "It will automatically generate a bunch of useful fields for each token object, such as with what type of word spaCy thinks it is (noun, adverb, etc), if it is a [stop word](https://en.wikipedia.org/wiki/Stop_words) (words which are filtered out as generally not useful for NLP tasks), if the word is in the language model's vocabulary, etc.\n",
    "\n",
    "Based on the post data, we will throw out tokens if one of the following applies:\n",
    "* it is a stop word\n",
    "* it is a proper noun, number, symbol, unknown, or punctuation other than a period\n",
    "* it is a new line\n",
    "* the word is not in our model's vocabulary\n",
    "\n",
    "After applying these steps we should be left with useful, common, non-unique or unusual words. This will make language processing for statistics easier in the next step and makes it highly unlikely that personally identifying information slipped through. \n",
    "\n",
    "In order to make this part of the notebook interactive, I have captured the output of the above stages into named tuples below, so you can try the NLP yourself. Note that the syntax is slightly different if you are using data pulled from a DB, and I have indicated where it differs.\n",
    "\n",
    "The results of applying this stage on the database can be found [here](https://github.com/johneddcooper/forumscrape/blob/master/db_refactoring/data_stg3_word_strip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm') # we use spaCy's en_core_web_sm language model for demonstration, however, we would use the md or lg models normally as they are more accurate and offer more features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' pulls best fruit\n",
      "' pulls best fruit\n",
      "' pulls best fruit\n",
      "' pulls best fruit\n",
      "Bears . Beets .\n",
      "Bears . Beets .\n",
      "Bears . Beets .\n",
      "Bears . Beets .\n",
      "Love\n",
      "Love\n",
      "Going na Eat Lot Peaches\n",
      "Going na Eat Lot Peaches\n",
      "like talk tomatoes\n",
      "like talk tomatoes\n",
      "like talk tomatoes\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sanitize post titles \n",
    "# Remove stop words, proper nouns, numbers, symbols, unknowns, punctuation other than periods, unknown words, and new lines.\n",
    "\n",
    "\n",
    "# If pulling from a database...\n",
    "collection = db['aggregate-posts-out_3']\n",
    "posts = collection.find({})\n",
    "# Note: if pulling the data from a database, post['thread_title'] needs to be used\n",
    "\n",
    "for post in posts:\n",
    "    content_doc = nlp(post['thread_title']) # Construct a new nlp doc, which automatically tokenizes and tags, returns a spaCy Doc object \n",
    "    # Go through each token in the post title, only keeping the ones that don't match our stripping rules\n",
    "    keep_tokens = [token.text for token in content_doc if not( # Iterating over a spaCy Doc returns the tokens within the Doc \n",
    "        token.is_stop or # remove stop words, or words that arn't useful for most NLP problems\n",
    "        (token.pos_ in ['PROPN','NUM','SYM','X','PUNCT'] and not token.text==\".\") or # remove proper nouns, numbers, symbols, unknowns, and all punctuation except periods, so we can delineate sentences.\n",
    "        #token.is_oov or # Remove words that are out of vocabulary (oov). We might lose some useful data, however, to be safe we take it out as we can't identify it.\n",
    "        token.is_space # Remove blank sentences\n",
    "    )]\n",
    "#    print(post.thread_title,keep_tokens) # will print once for each post, so there will be duplicates\n",
    "    print(' '.join(keep_tokens))\n",
    "# Code to update the changes to a DB, run from within the for loop\n",
    "    collection.update_many(\n",
    "       {'thread_title':post['thread_title']},\n",
    "       {'$set':{'thread_title':str(' '.join(keep_tokens))}}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knows knows apples enlightened pulls best . fruits yucky vegetables inferior . Discuss .\n",
      "pulls best . best ' pulls ripe juicy taste sooooo good . match eat apples day day doctors damned\n",
      "real . Apples gross . worst fruits bad . knows veggies power food . week apple sauce .\n",
      "knows grapes best filthy casual Enjoy cyanide balls\n",
      ".\n",
      ". . theft joke Millions families suffer year\n",
      "storms\n",
      "Oh funny .\n",
      "head tomatoes .\n",
      ". pearfect couple .\n",
      "Peaches better . Peaches best cabbage worst end story . & yuck\n",
      "Corn cob rows home placeeee belonggg ROWWWSS\n",
      "squash smile\n",
      "Lol tomatoes squash fruits compost veggie master plate forum .\n",
      "Na tomatoes squash fruits . Acidic tomatoes dirty squash like dirty tomatoes stay sweet fruit forum . Like yucky lettuce belong vegetable forum aka compost .\n",
      "\n",
      "\n",
      "\n",
      "Banana\n",
      "\n",
      "\n",
      "\n",
      "Banana\n",
      "\n",
      "s .\n",
      "\n",
      "Orange\n",
      "GLAD DIDN'T\n",
      "thought pull fast little bitch know graduated class involved numerous secret raids confirmed spills . trained vanilla warfare ripener entire farmed forces . target grocery aisle . wipe fuck precision likes seen mark fucking words . think away saying shit Internet Think fucker . speak contacting secret network pies IP traced right better prepare storm maggot . storm wipes pathetic little thing life . fucking dead kid . anytime eat ways bare hands . extensively trained organic combat access entire pesticide arsenal use extent wipe miserable ass face continent little shit . known appleholey retribution little clever comment bring maybe held fucking tongue . paying price goddamn idiot . shit fury drown . lettuce head fucking dead kiddo .\n",
      "boiled damn .\n"
     ]
    }
   ],
   "source": [
    "# Sanitize post contents\n",
    "# Remove stop words, proper nouns, numbers, symbols, unknowns, punctuation other than periods, unknown words, and new lines.\n",
    "\n",
    "# If pulling from a database...\n",
    "collection = db['aggregate-posts-out_3']\n",
    "posts = collection.find({})\n",
    "# Note: if pulling the data from a database, post['thread_title'] needs to be used\n",
    "\n",
    "for post in posts:\n",
    "    content_doc = nlp(post['content']) \n",
    "    # Go through each token in the post title, only keeping the ones that don't match our stripping rules\n",
    "    keep_tokens = [token.text for token in content_doc if not( \n",
    "        token.is_stop or # remove stop words, or words that arn't useful for most NLP problems\n",
    "        (token.pos_ in ['PROPN','NUM','SYM','X','PUNCT'] and not token.text==\".\") or # remove proper nouns, numbers, symbols, unknowns, and all puncuation expect periods, so we can deliniate sentences.\n",
    "        #token.is_oov or # Remove words that are out of vocabulary (oov). We might lose some useful data, however to be safe we take it out as we can't identify it.\n",
    "        token.is_space # Remove blank sentances\n",
    "    )]\n",
    "    print(' '.join(keep_tokens))\n",
    "# Code to update the changes to a DB, run within the for loop\n",
    "    collection.update_one(\n",
    "       {'_id':post['_id']},\n",
    "       {'$set':{'content':str(' '.join(keep_tokens))}}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration, lets see all of the tokens that we stripped out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "stripped_tokens = set()\n",
    "\n",
    "for post in posts:\n",
    "    content_doc = nlp(post.content) \n",
    "    throwaway_tokens = [token for token in content_doc if  \n",
    "        # token.is_stop or # remove stop words, or words that aren't useful for most NLP problems\n",
    "        (token.pos_ in ['PROPN','NUM','SYM','X','PUNCT'] and not token.text==\".\") or # remove proper nouns, numbers, symbols, unknowns, and all punctuation except periods, so we can delineate sentences.\n",
    "        token.is_oov or # Remove words that are out of vocabulary (oov). We might lose some useful data, however, to be safe we take it out as we can't identify it.\n",
    "        token.is_space # Remove blank sentences\n",
    "    ]\n",
    "    for token in throwaway_tokens:\n",
    "        stripped_tokens.add(token)\n",
    "        \n",
    "        \n",
    "print(stripped_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, we have seen:\n",
    "* Refactoring a DB using MongoDB aggregation pipelines\n",
    "* Assigning a unique number to users and replacing them in the DB\n",
    "* Stripping out unwanted words from the DB using simple language processing with spaCy\n",
    "\n",
    "In the next post, we will dive more into spaCy and NLP, and how we can derive some useful information from our cleaned dataset.\n",
    "\n",
    "If you have any comments or recommendations, please let me know by leaving a comment at https://learningautomaton.ca/forum-scrape-project---db-editing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre><code>if can_learn: \n",
    "    learn()</code></pre>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
