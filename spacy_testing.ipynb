{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Automaton\n",
    "Forum Scrape Project\n",
    "Information Processing\n",
    "\n",
    "\n",
    "Goals:\n",
    "Associate words to adjectives via rules\n",
    "Associate words to adjectives by sentence\n",
    "Associate user to Nouns\n",
    "Instances of fruits, veggies\n",
    "do basic sentiment analasys using 1, 2, 3, depending on effectivness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruit_match_words = set(['pulls','fruit','apple','apricot','banana','cantaloupe','cherry','date','citrus','avocado','carissa','guava','cherry','citron','clementine','crabapple','grape','grapefruit','honeydew','lemon','lime','orange','mandarin','mango','papaya','peach','pear','pineapple','plantain','plum','pomelo','tangarine','watermelon'])\n",
    "vegetable_match_words =  set(['vegetable','artichoke','eggplant','asparagu','broccoli','cabbage','cauliflower','celery','spinach','lettuce','onion','beet','carrot','potato','yam','turnip','squash','tomato','watercress'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "from pprint import pprint\n",
    "from pymongo import MongoClient\n",
    "client = MongoClient('192.168.2.70', 27017)\n",
    "db=client['fruitsandveggies']\n",
    "collection = db['aggregate-posts-out_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "# TODO test lg vs sm\n",
    "# TODO test spacy core in binder\n",
    "nlp_sm = spacy.load('en_core_web_sm')\n",
    "nlp_md = spacy.load('en_core_web_md')\n",
    "nlp_lg = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = \"Apples are the best. Us enlightened who call them, 'pulls love them more than any other fruit. 'Pulls are tasty, sweet, and crisp. All other fruits and those yucky vegetables are inferior. Discuss.. Ya, 'pulls are the best. 'Pulls are ripe, juicy, and taste sooooo good. Nothing else can match! I'd eat apples every day all day if I could, doctors be damned!. Yall get real. Apples are gross. The worst of the fruits, which are all bad. Everyone knows that veggies are powerfood. Get your week apple-sauce out of here.. Everyone knows grapes are superior you filthy casual. Enjoy your cyanide balls. Battlestar Galactica.. NO ONE READ THE ABOVE. THAT IS NOT ME.  Identity theft is not a joke, Jim! Millions of families suffer every year!. MICHAEL!  *storms off*. Oh, that's funny.  MICHAEL. From my head tomatoes.. Never Leaf Me.  We make a pearfect couple.. Peaches are better than all others. Peaches are the best, cabbage is the worst, end of story.  Smelly Cabbage &gt;.&lt; yuck. Corn cob rows!  Take me home,  to the placeeee,  I belonggg!!!!  WEST INDIANAAAAA  BUTTER MAMAAAAA  TAKE ME HOMEEEEE  CORN BOB ROWWWSS. If a squash can make you smile.... Lol, tomatoes and squash are both fruits! Get this compost out of the veggie master plate forum. Na, tomatoes and squash are fruits in name only. Acidic tomatoes and dirty squash, just like dirty tomatoes, should stay out of our sweet fruit forum. Like yucky lettuce, they belong in the vegetable forum, aka the compost.. Knock Knock. Who's There?. Banana. Banana who?. Knock Knock.... Who's There?... Banana. Banana who?. Knock Knock. Whos. ... There..... Orange!. Orange who?. ORANGE YOU GLAD I DIDN'T SAY BANANA??!?. You thought you'd pull a fast one on me, you little bitch? I'll have you know I graduated top of my class in the Tasty Peels, and I've been involved in numerous secret raids on Al-Quinoa, and I have over 300 confirmed spills. I am trained in vanilla warfare and I'm the top ripener in the entire US farmed forces. You are nothing to me but just another target grocery aisle. I will wipe you the fuck out with precision the likes of which has never been seen before on this Earth, mark my fucking words. You think you can get away with saying that shit to me over the Internet? Think again, fucker. As we speak I am contacting my secret network of pies across the USA and your IP is being traced right now so you better prepare for the storm, maggot. The storm that wipes out the pathetic little thing you call your life. You're fucking dead, kid. I can be anywhere, anytime, and I can eat you in over seven hundred ways, and that's just with my bare hands. Not only am I extensively trained in organic combat, but I have access to the entire pesticide arsenal of the United States Soybean Crops and I will use it to its full extent to wipe your miserable ass off the face of the continent, you little shit. If only you could have known what appleholey retribution your little \\\"clever\\\" comment was about to bring down upon you, maybe you would have held your fucking tongue. But you couldn't, you didn't, and now you're paying the price, you goddamn idiot. I will shit fury all over you and you will drown in it. Your lettuce head is fucking dead, kiddo.. Get boiled, damn.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in nlp_lg.Defaults.stop_words:\n",
    "    lex = nlp_lg.vocab[word]\n",
    "    lex.is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp_lg(raw)\n",
    "keep_tokens = [token.text for token in doc if not( # Iterating over a spaCy Doc returns the tokens within the Doc \n",
    "        token.is_stop or # remove stop words, or words that arn't useful for most NLP problems\n",
    "        (token.pos_ in ['PROPN','NUM','SYM','X','PUNCT'] and not token.text==\".\") or # remove proper nouns, numbers, symbols, unknowns, and all punctuation except periods, so we can delineate sentences.\n",
    "        #token.is_oov or # Remove words that are out of vocabulary (oov). We might lose some useful data, however, to be safe we take it out as we can't identify it.\n",
    "        token.is_space # Remove blank sentences\n",
    "    )]\n",
    "#    print(post.thread_title,keep_tokens) # will print once for each post, so there will be duplicates\n",
    "\n",
    "\n",
    "# doc_strip = nlp_lg(' '.join(keep_tokens))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "##--NEED TO INSTALL SPACY 2.1 TO USE MATCH ON LEMMA--##\n",
    "\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "class EntityMatcher(object):\n",
    "\n",
    "    def __init__(self, nlp, terms, matcher_name, label):\n",
    "        patterns = [nlp.make_doc(text) for text in terms]\n",
    "        self.matcher = PhraseMatcher(nlp.vocab,attr=\"LEMMA\")\n",
    "        self.matcher.add(label, None, *patterns)\n",
    "        self.name = matcher_name\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        matches = self.matcher(doc)\n",
    "        for match_id, start, end in matches:\n",
    "            span = Span(doc, start, end, label=match_id)\n",
    "            doc.ents = list(doc.ents) + [span]\n",
    "        return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'attr'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-66ba2dba61af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfruit_matcher\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEntityMatcher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnlp_lg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfruit_match_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"fruit_matcher\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"FRUIT\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mvegetable_matcher\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEntityMatcher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnlp_lg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvegetable_match_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"vegerable_matcher\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"VEGETABLE\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnlp_lg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfruit_matcher\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnlp_lg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvegetable_matcher\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-61-3750a29bf4b2>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, nlp, terms, matcher_name, label)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatcher_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mpatterns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mterms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatcher\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPhraseMatcher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mattr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"LEMMA\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mpatterns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatcher_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mmatcher.pyx\u001b[0m in \u001b[0;36mspacy.matcher.PhraseMatcher.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'attr'"
     ]
    }
   ],
   "source": [
    "\n",
    "fruit_matcher = EntityMatcher(nlp_lg, fruit_match_words, \"fruit_matcher\", \"FRUIT\")\n",
    "vegetable_matcher = EntityMatcher(nlp_lg, vegetable_match_words, \"vegerable_matcher\", \"VEGETABLE\")\n",
    "nlp_lg.add_pipe(fruit_matcher)\n",
    "nlp_lg.add_pipe(vegetable_matcher)\n",
    "\n",
    "doc_strip = nlp_lg(' '.join(keep_tokens))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('pulls', 'FRUIT'), ('fruit', 'FRUIT'), ('pulls', 'FRUIT'), ('day day', 'DATE'), ('Yall', 'PERSON'), ('Get week', 'DATE'), ('apple', 'FRUIT'), ('Millions', 'CARDINAL'), ('cabbage', 'VEGETABLE'), ('INDIANAAAAA', 'ORG'), ('squash', 'VEGETABLE'), ('squash', 'VEGETABLE'), ('squash', 'VEGETABLE'), ('squash', 'VEGETABLE'), ('fruit', 'FRUIT'), ('lettuce', 'VEGETABLE'), ('vegetable', 'VEGETABLE'), (\"DIDN'T\", 'PERSON'), ('lettuce', 'VEGETABLE')]\n"
     ]
    }
   ],
   "source": [
    "#for token in doc_strip:\n",
    "#    print('{}, {}, {}'.format(token.text, token.lemma_, token.pos_))\n",
    "print([(ent.text, ent.label_) for ent in doc_strip.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9076452796594085796 NounIsAdv 0 2 Apples best\n",
      "16820485401210444574 NounIsAdj 15 17 fruits yucky\n",
      "6136520347023808161 AdjNoun 16 18 yucky vegetables\n",
      "16820485401210444574 NounIsAdj 17 19 vegetables inferior\n",
      "6136520347023808161 AdjNoun 27 29 juicy taste\n",
      "9076452796594085796 NounIsAdv 28 30 taste sooooo\n",
      "16820485401210444574 NounIsAdj 45 47 Apples gross\n",
      "6136520347023808161 AdjNoun 49 51 worst fruits\n",
      "16820485401210444574 NounIsAdj 50 52 fruits bad\n",
      "6136520347023808161 AdjNoun 66 68 filthy casual\n",
      "6136520347023808161 AdjNoun 88 90 year!. storms\n",
      "9076452796594085796 NounIsAdv 98 100 tomatoes Never\n",
      "16820485401210444574 NounIsAdj 109 111 Peaches best\n",
      "6136520347023808161 AdjNoun 110 112 best cabbage\n",
      "16820485401210444574 NounIsAdj 111 113 cabbage worst\n",
      "6136520347023808161 AdjNoun 112 114 worst end\n",
      "6136520347023808161 AdjNoun 149 151 Acidic tomatoes\n",
      "16820485401210444574 NounIsAdj 150 152 tomatoes dirty\n",
      "6136520347023808161 AdjNoun 151 153 dirty squash\n",
      "6136520347023808161 AdjNoun 154 156 dirty tomatoes\n",
      "6136520347023808161 AdjNoun 157 159 sweet fruit\n",
      "6136520347023808161 AdjNoun 162 164 yucky lettuce\n",
      "9076452796594085796 NounIsAdv 166 168 forum aka\n",
      "6136520347023808161 AdjNoun 200 202 little bitch\n",
      "6136520347023808161 AdjNoun 213 215 secret raids\n",
      "16820485401210444574 NounIsAdj 226 228 ripener entire\n",
      "6136520347023808161 AdjNoun 238 240 fuck precision\n",
      "16820485401210444574 NounIsAdj 242 244 mark fucking\n",
      "6136520347023808161 AdjNoun 243 245 fucking words\n",
      "6136520347023808161 AdjNoun 259 261 secret network\n",
      "6136520347023808161 AdjNoun 273 275 little thing\n",
      "6136520347023808161 AdjNoun 280 282 dead kid\n",
      "6136520347023808161 AdjNoun 289 291 bare hands\n",
      "6136520347023808161 AdjNoun 296 298 organic combat\n",
      "6136520347023808161 AdjNoun 300 302 entire pesticide\n",
      "6136520347023808161 AdjNoun 307 309 miserable ass\n",
      "16820485401210444574 NounIsAdj 310 312 continent little\n",
      "6136520347023808161 AdjNoun 311 313 little shit\n",
      "6136520347023808161 AdjNoun 316 318 appleholey retribution\n",
      "16820485401210444574 NounIsAdj 317 320 retribution little clever\n",
      "6136520347023808161 AdjNoun 319 321 clever comment\n",
      "6136520347023808161 AdjNoun 324 326 fucking tongue\n",
      "16820485401210444574 NounIsAdj 332 334 price goddamn\n",
      "6136520347023808161 AdjNoun 333 335 goddamn idiot\n",
      "6136520347023808161 AdjNoun 341 343 Your lettuce\n",
      "6136520347023808161 AdjNoun 345 347 dead kiddo\n"
     ]
    }
   ],
   "source": [
    "doc = doc_strip\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp_lg.vocab)\n",
    "\n",
    "noun_is_adj_plus = [{\"POS\": \"NOUN\"}, {\"POS\": \"ADV\", \"OP\": \"*\"}, {\"POS\": \"DET\", \"OP\": \"*\"},\n",
    "           {\"POS\": \"ADJ\", \"OP\": \"+\"}]\\\n",
    "\n",
    "noun_is_adj = [{\"POS\": \"NOUN\"}, {\"POS\": \"ADJ\", \"OP\": \"*\"}, {\"POS\": \"ADJ\", \"OP\": \"+\"}]\n",
    "noun_is_adv = [{\"POS\": \"NOUN\"}, {\"POS\": \"ADV\", \"OP\": \"+\"}]\n",
    "\n",
    "adj_noun = [ {\"POS\": \"ADJ\"},{\"POS\": \"NOUN\"}]\n",
    "\n",
    "matcher.add(\"NounIsAdj\", None, noun_is_adj)  # add pattern\n",
    "matcher.add(\"NounIsAdv\", None, noun_is_adv)  # add pattern\n",
    "matcher.add(\"AdjNoun\", None, adj_noun)\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "fruit_words = []\n",
    "veggie_words = []\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp_lg.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    #if not set([token.lemma_ for token in span]).isdisjoint(fruit_match_words):\n",
    "    #    print(span.text)\n",
    "    \n",
    "        \n",
    "    #if set(span.text.split(' ')) & fruit_match_words:\n",
    "    #    print(span.text)\n",
    "\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fruit mentions: 20\n",
      "Veg mentions: 15\n"
     ]
    }
   ],
   "source": [
    "doc = nlp_sm(raw)\n",
    "from collections import defaultdict\n",
    "word_count = defaultdict(int)\n",
    "for token in doc:\n",
    "    if token.lemma_ in fruit_match_words.union(vegetable_match_words):\n",
    "        word_count[token.lemma_] += 1\n",
    "\n",
    "fruit_count = sum([word_count[key] for key in word_count.keys() if key in fruit_match_words])\n",
    "veg_count = sum([word_count[key] for key in word_count.keys() if key in vegetable_match_words])\n",
    "\n",
    "print(\"Fruit mentions: {}\".format(fruit_count))\n",
    "print(\"Veg mentions: {}\".format(veg_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('apple', 'good'), ('fruit', 'yucky'), ('apple', 'gross'), ('fruit', 'which'), ('grape', 'superior'), ('peach', 'good'), ('peach', 'good')]\n"
     ]
    }
   ],
   "source": [
    "#posts = collection.find({})\n",
    "\n",
    "noun_adj_pairs = []\n",
    "\n",
    "for sent in doc.sents:\n",
    "    for i,token in enumerate(sent):\n",
    "        if token.pos_ not in ('NOUN','PROPN', 'ADJ'):\n",
    "            continue\n",
    "        for j in range(i+1,len(sent)):\n",
    "            if (token.pos_ in ('NOUN','PROPN') and sent[j].pos_ == 'ADJ'):\n",
    "                noun_adj_pairs.append((token,sent[j]))\n",
    "                break\n",
    "            if (token.pos_ == 'ADJ' and sent[j].pos_ in ('NOUN','PROPN')):\n",
    "           #    noun_adj_pairs.append((sent[j], token))\n",
    "                break\n",
    "    \n",
    "fruit_pairs = [(pair[0].lemma_,pair[1].lemma_) for pair in noun_adj_pairs if pair[0].lemma_ in fruit_match_words]\n",
    "veg_pairs = [(pair[0].lemma_,pair[1].lemma_) for pair in noun_adj_pairs if pair[0].lemma_ in vegetable_match_words]\n",
    "\n",
    "print(fruit_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "ruler = EntityRuler(nlp_lg)\n",
    "patterns = [{\"label\": \"FRUIT\", \"pattern\": [{\"lower\":\"apple\"}]},\n",
    "            {\"label\": \"FRUIT\", \"pattern\": [{\"lower\":\"grape\"}]},\n",
    "           ]\n",
    "ruler.add_patterns(patterns)\n",
    "nlp.add_pipe(ruler)\n",
    "\n",
    "threads = collection.find({\"title\":\"Apples ('pulls) are the best fruit\"})\n",
    "threads = [thread['posts'] for thread in threads]\n",
    "posts = [post for thread in threads for post in thread]\n",
    "contents = [post['content'] for post in posts]\n",
    "pprint(contents)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
