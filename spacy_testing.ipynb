{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Automaton\n",
    "Forum Scrape Project\n",
    "Information Processing\n",
    "\n",
    "\n",
    "Goals:\n",
    "Associate words to adjectives via rules\n",
    "Associate words to adjectives by sentence\n",
    "Associate user to Nouns\n",
    "Instances of fruits, veggies\n",
    "do basic sentiment analasys using 1, 2, 3, depending on effectivness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruit_match_words = set(['pulls','fruit','apple','apricot','banana','cantaloupe','cherry','date','citrus','avocado','carissa','guava','cherry','citron','clementine','crabapple','grape','grapefruit','honeydew','lemon','lime','orange','mandarin','mango','papaya','peach','pear','pineapple','plantain','plum','pomelo','tangarine','watermelon'])\n",
    "vegetable_match_words =  set(['vegetable','artichoke','eggplant','asparagu','broccoli','cabbage','cauliflower','celery','spinach','lettuce','onion','beet','carrot','potato','yam','turnip','squash','tomato','watercress'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "from pprint import pprint\n",
    "from pymongo import MongoClient\n",
    "client = MongoClient('192.168.2.70', 27017)\n",
    "db=client['fruitsandveggies']\n",
    "collection = db['aggregate-posts-out_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "# TODO test lg vs sm\n",
    "# TODO test spacy core in binder\n",
    "\n",
    "#nlp_md = spacy.load('en_core_web_md')\n",
    "nlp_lg = spacy.load('en_core_web_lg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_sm = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = nlp_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in nlp.Defaults.stop_words:\n",
    "    lex = nlp.vocab[word]\n",
    "    lex.is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##--NEED TO INSTALL SPACY 2.1 TO USE MATCH ON LEMMA--##\n",
    "\n",
    "\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "class EntityMatcher(object):\n",
    "\n",
    "    def __init__(self, nlp, terms, matcher_name, label):\n",
    "        patterns = [nlp(text) for text in terms]\n",
    "        self.matcher = PhraseMatcher(nlp.vocab, attr=\"LEMMA\")\n",
    "        self.matcher.add(label, None, *patterns)\n",
    "        self.name = matcher_name\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        matches = self.matcher(doc)\n",
    "        for match_id, start, end in matches:\n",
    "            span = Span(doc, start, end, label=match_id)\n",
    "            doc.ents = list(doc.ents) + [span]\n",
    "        return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.pipes.Tagger at 0x1a405d0b6a0>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x1a407a2fd68>),\n",
       " ('fruit_matcher', <__main__.EntityMatcher at 0x1a405c50278>),\n",
       " ('vegetable_matcher', <__main__.EntityMatcher at 0x1a405c50240>)]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fruit_matcher = EntityMatcher(nlp, fruit_match_words, \"fruit_matcher\", \"FRUIT\")\n",
    "\n",
    "vegetable_matcher = EntityMatcher(nlp, vegetable_match_words, \"vegetable_matcher\", \"VEGETABLE\")\n",
    "\n",
    "#nlp.disable_pipes(\"ner\")\n",
    "#nlp.add_pipe(fruit_matcher)\n",
    "#nlp.add_pipe(vegetable_matcher)\n",
    "\n",
    "#doc_strip = nlp(' '.join(doc))\n",
    "nlp.pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = \"Apples are the best. Us enlightened who call them, 'pulls love them more than any other fruit. 'Pulls are tasty, sweet, and crisp. All other fruits and those yucky vegetables are inferior. Discuss.. Ya, 'pulls are the best. 'Pulls are ripe, juicy, and taste sooooo good. Nothing else can match! I'd eat apples every day all day if I could, doctors be damned!. Yall get real. Apples are gross. The worst of the fruits, which are all bad. Everyone knows that veggies are powerfood. Get your weak apple sauce out of here.. Everyone knows grapes are superior you filthy casual. Enjoy your cyanide balls. Battlestar Galactica.. NO ONE READ THE ABOVE. THAT IS NOT ME.  Identity theft is not a joke, Jim! Millions of families suffer every year!. MICHAEL!  *storms off*. Oh, that's funny.  MICHAEL. From my head tomatoes.. Never Leaf Me.  We make a pearfect couple.. Peaches are better than all others. Peaches are the best, cabbage is the worst, end of story.  Smelly Cabbage; yuck. Corn cob rows!  Take me home,  to the placeeee,  I belonggg!!!!  WEST INDIANAAAAA  BUTTER MAMAAAAA  TAKE ME HOMEEEEE  CORN BOB ROWWWSS. If a squash can make you smile.... Lol, tomatoes and squash are both fruits! Get this compost out of the veggie master plate forum. Na, tomatoes and squash are fruits in name only. Acidic tomatoes and dirty squash, just like dirty tomatoes, should stay out of our sweet fruit forum. Like yucky lettuce, they belong in the vegetable forum, aka the compost.. Knock Knock. Who's There?. Banana. Banana who?. Knock Knock.... Who's There?... Banana. Banana who?. Knock Knock. Whos. ... There..... Orange!. Orange who?. ORANGE YOU GLAD I DIDN'T SAY BANANA??!?. You thought you'd pull a fast one on me, you little bitch? I'll have you know I graduated top of my class in the Tasty Peels, and I've been involved in numerous secret raids on Al-Quinoa, and I have over 300 confirmed spills. I am trained in vanilla warfare and I'm the top ripener in the entire US farmed forces. You are nothing to me but just another target grocery aisle. I will wipe you the fuck out with precision the likes of which has never been seen before on this Earth, mark my fucking words. You think you can get away with saying that shit to me over the Internet? Think again, fucker. As we speak I am contacting my secret network of pies across the USA and your IP is being traced right now so you better prepare for the storm, maggot. The storm that wipes out the pathetic little thing you call your life. You're fucking dead, kid. I can be anywhere, anytime, and I can eat you in over seven hundred ways, and that's just with my bare hands. Not only am I extensively trained in organic combat, but I have access to the entire pesticide arsenal of the United States Soybean Crops and I will use it to its full extent to wipe your miserable ass off the face of the continent, you little shit. If only you could have known what appleholey retribution your little \\\"clever\\\" comment was about to bring down upon you, maybe you would have held your fucking tongue. But you couldn't, you didn't, and now you're paying the price, you goddamn idiot. I will shit fury all over you and you will drown in it. Your lettuce head is fucking dead, kiddo.. Get boiled, damn.\"\n",
    "#raw = \"apple. bananas. cars. turtles. carrot.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "keep_tokens = [token.text for token in doc if not( # Iterating over a spaCy Doc returns the tokens within the Doc \n",
    "        token.is_stop or # remove stop words, or words that arn't useful for most NLP problems\n",
    "        (token.pos_ in ['PROPN','NUM','SYM','X','PUNCT'] and not token.text==\".\") or # remove proper nouns, numbers, symbols, unknowns, and all punctuation except periods, so we can delineate sentences.\n",
    "        #token.is_oov or # Remove words that are out of vocabulary (oov). We might lose some useful data, however, to be safe we take it out as we can't identify it.\n",
    "        token.is_space # Remove blank sentences\n",
    "    )]\n",
    "#    print(post.thread_title,keep_tokens) # will print once for each post, so there will be duplicates\n",
    "\n",
    "\n",
    "doc_strip = nlp(' '.join(keep_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_full = nlp(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = doc_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'fruit': 6, 'pull': 5, 'apple': 4, 'peach': 2, 'grape': 1})\n",
      "Counter({'squash': 4, 'tomato': 4, 'vegetable': 2, 'lettuce': 2, 'cabbage': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "fruits = Counter()\n",
    "veggies = Counter()\n",
    "for ent in doc.ents:\n",
    "    (fruits if ent.label_ == \"FRUIT\" else veggies).update([ent.lemma_])\n",
    "print(fruits)\n",
    "print(veggies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc_strip:\n",
    "   print('{}, {}, {}'.format(token.text, token.lemma_, token.norm_, token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9076452796594085796 NounIsAdv 0 2 Apples best\n",
      "16820485401210444574 NounIsAdj 8 10 Pulls tasty\n",
      "16820485401210444574 NounIsAdj 13 15 fruits yucky\n",
      "6136520347023808161 AdjNoun 14 16 yucky vegetables\n",
      "16820485401210444574 NounIsAdj 15 17 vegetables inferior\n",
      "9076452796594085796 NounIsAdv 19 21 pulls best\n",
      "16820485401210444574 NounIsAdj 22 24 Pulls ripe\n",
      "16820485401210444574 NounIsAdj 37 39 Apples gross\n",
      "6136520347023808161 AdjNoun 40 42 worst fruits\n",
      "16820485401210444574 NounIsAdj 41 43 fruits bad\n",
      "16820485401210444574 NounIsAdj 52 54 grapes superior\n",
      "9076452796594085796 NounIsAdv 80 82 Peaches better\n",
      "16820485401210444574 NounIsAdj 83 85 Peaches best\n",
      "6136520347023808161 AdjNoun 84 86 best cabbage\n",
      "16820485401210444574 NounIsAdj 85 87 cabbage worst\n",
      "6136520347023808161 AdjNoun 118 120 Acidic tomatoes\n",
      "6136520347023808161 AdjNoun 123 125 dirty tomatoes\n",
      "6136520347023808161 AdjNoun 126 128 sweet fruit\n",
      "6136520347023808161 AdjNoun 131 133 yucky lettuce\n",
      "9076452796594085796 NounIsAdv 152 154 pull fast\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "doc = doc_strip\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "noun_is_adj_plus = [{\"ENT_TYPE\": {\"IN\": [\"FRUIT\",\"VEGETABLE\"]}}, {\"POS\": \"ADV\", \"OP\": \"*\"}, {\"POS\": \"DET\", \"OP\": \"*\"},\n",
    "            {\"POS\": \"ADJ\", \"OP\": \"+\"}]\\\n",
    "\n",
    "noun_is_adj = [{\"ENT_TYPE\": {\"IN\": [\"FRUIT\",\"VEGETABLE\"]}}, {\"POS\": \"ADJ\"}]\n",
    "noun_is_adv = [{\"ENT_TYPE\": {\"IN\": [\"FRUIT\",\"VEGETABLE\"]}}, {\"POS\": \"ADV\"}]\n",
    "\n",
    "adj_noun = [ {\"POS\": \"ADJ\"},{\"ENT_TYPE\": {\"IN\": [\"FRUIT\",\"VEGETABLE\"]}}]\n",
    "\n",
    "matcher.add(\"NounIsAdj\", None, noun_is_adj)  # add pattern\n",
    "matcher.add(\"NounIsAdv\", None, noun_is_adv)  # add pattern\n",
    "matcher.add(\"AdjNoun\", None, adj_noun)\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "fruit_words = []\n",
    "veggie_words = []\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    #if not set([token.lemma_ for token in span]).isdisjoint(fruit_match_words):\n",
    "    #    print(span.text)\n",
    "    \n",
    "        \n",
    "    #if set(span.text.split(' ')) & fruit_match_words:\n",
    "    #    print(span.text)\n",
    "\n",
    "    print(match_id, string_id, start, end, span.text)\n",
    "print(len(matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fruit mentions: 13\n",
      "Veg mentions: 14\n"
     ]
    }
   ],
   "source": [
    "doc = doc_strip\n",
    "from collections import defaultdict\n",
    "word_count = defaultdict(int)\n",
    "for token in doc:\n",
    "    if token.lemma_ in fruit_match_words.union(vegetable_match_words):\n",
    "        word_count[token.lemma_] += 1\n",
    "\n",
    "fruit_count = sum([word_count[key] for key in word_count.keys() if key in fruit_match_words])\n",
    "veg_count = sum([word_count[key] for key in word_count.keys() if key in vegetable_match_words])\n",
    "\n",
    "print(\"Fruit mentions: {}\".format(fruit_count))\n",
    "print(\"Veg mentions: {}\".format(veg_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('fruit', 'yucky'), ('apple', 'real'), ('apple', 'gross'), ('fruit', 'bad'), ('apple', 'superior'), ('grape', 'superior'), ('peach', 'good'), ('fruit', 'veggie')]\n",
      "[('vegetable', 'inferior'), ('cabbage', 'bad'), ('tomato', 'dirty'), ('squash', 'dirty'), ('tomato', 'sweet'), ('lettuce', 'dead')]\n"
     ]
    }
   ],
   "source": [
    "#posts = collection.find({})\n",
    "doc = doc_strip\n",
    "noun_adj_pairs = []\n",
    "\n",
    "for sent in doc.sents:\n",
    "    for i,token in enumerate(sent):\n",
    "        if token.ent_type_ not in ('FRUIT','VEGETABLE'):\n",
    "            continue\n",
    "        for j in range(i+1,len(sent)):\n",
    "            if (sent[j].pos_ == 'ADJ'):\n",
    "                noun_adj_pairs.append((token,sent[j]))\n",
    "                break\n",
    "    \n",
    "fruit_pairs = [(pair[0].lemma_,pair[1].lemma_) for pair in noun_adj_pairs if pair[0].lemma_ in fruit_match_words]\n",
    "veg_pairs = [(pair[0].lemma_,pair[1].lemma_) for pair in noun_adj_pairs if pair[0].lemma_ in vegetable_match_words]\n",
    "\n",
    "print(fruit_pairs)\n",
    "print(veg_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
