{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "from pprint import pprint\n",
    "from pymongo import MongoClient\n",
    "client = MongoClient('192.168.2.70', 27017)\n",
    "db=client['fruitsandveggies']\n",
    "collection = db['aggregate-posts-out_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "# TODO test lg vs sm\n",
    "# TODO test spacy core in binder\n",
    "#nlp_sm = spacy.load('en_core_web_sm')\n",
    "nlp_md = spacy.load('en_core_web_md')\n",
    "#nlp_lg = spacy.load('en_core_web_lg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = nlp_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in nlp.Defaults.stop_words:\n",
    "    lex = nlp.vocab[word]\n",
    "    lex.is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = db['aggregate-posts-out_2']\n",
    "posts = collection.find({})\n",
    "# Note: if pulling the data from a database, post['thread_title'] needs to be used\n",
    "\n",
    "post_tuples = []\n",
    "for post in posts:\n",
    "   post_tuples.append((post['_id'],post['user'],nlp(post['content'])))\n",
    "\n",
    "post_tuples.append((_,_,nlp(\"i think that i'm retarded\")))\n",
    "\n",
    "# for post in posts:\n",
    "#     content_doc = nlp(post.thread_title) # Construct a new nlp doc, which automatically tokenizes and tags, returns a spaCy Doc object \n",
    "#     # Go through each token in the post title, only keeping the ones that don't match our stripping rules\n",
    "#     keep_tokens = [token for token in content_doc if not( # Iterating over a spaCy Doc returns the tokens within the Doc \n",
    "#         token.is_stop or # remove stop words, or words that arn't useful for most NLP problems\n",
    "#         (token.pos_ in ['PROPN','NUM','SYM','X','PUNCT'] and not token.text==\".\") or # remove proper nouns, numbers, symbols, unknowns, and all punctuation except periods, so we can delineate sentences.\n",
    "#         #token.is_oov or # Remove words that are out of vocabulary (oov). We might lose some useful data, however, to be safe we take it out as we can't identify it.\n",
    "#         token.is_space # Remove blank sentences\n",
    "#     )]\n",
    "#     print(post.thread_title,keep_tokens) # will print once for each post, so there will be duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pulls NOUN\n",
      "are VERB\n",
      "the DET\n",
      "best ADJ\n",
      "apples NOUN\n",
      "are VERB\n",
      "the DET\n",
      "best ADJ\n",
      "filthy ADJ\n",
      "casual NOUN\n",
      "corn NOUN\n",
      "cob NOUN\n",
      "oh INTJ\n",
      "that DET\n",
      "â€™s VERB\n",
      "funny ADJ\n",
      "I PRON\n",
      "'m VERB\n",
      "retarded ADJ\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "match_docs = []\n",
    "with open('search_phrases.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        match_docs.append(nlp(row[0]))\n",
    "match_docs.append(nlp(\"I'm retarded\"))\n",
    "for doc in match_docs:\n",
    "    for token in doc:\n",
    "        print(token, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ObjectId('5cc2605bcddd5b478f824c6e'), 9, So everyone who knows what 's what knows that apples or as us enlightened call them pulls are the best . All other fruits and those yucky vegetables are inferior . Discuss .)\n",
      "(ObjectId('5cc2605bcddd5b478f824c6f'), 14, pulls are the best . The best ' pulls are ripe juicy and taste sooooo good . Nothing else can match I 'd eat apples every day all day if I could doctors be damned)\n",
      "(ObjectId('5cc2605bcddd5b478f824c71'), 3, Everyone knows grapes are the best you filthy casual Enjoy your cyanide balls)\n",
      "(ObjectId('5cc2605bcddd5b478f824c79'), 11, Corn cob rows Take me home to the placeeee I belonggg TAKE ME ROWWWSS)\n",
      "(False, False, i think that i'm retarded)\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "def on_match(matcher, doc, id, matches):\n",
    "    pass\n",
    "    #print('Matched!', matches, doc.text, id)\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab,attr='LOWER')\n",
    "    \n",
    "for match_doc in match_docs:\n",
    "    matcher.add(\"REEEEE\", None, match_doc)\n",
    "matches = []\n",
    "for post_tuple in post_tuples:\n",
    "    if(len(matcher(post_tuple[2]))):\n",
    "        matches.append(post_tuple)\n",
    "\n",
    "for match in matches:\n",
    "    print(match)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(raw)\n",
    "keep_tokens = [token.text for token in doc if not( # Iterating over a spaCy Doc returns the tokens within the Doc \n",
    "        token.is_stop or # remove stop words, or words that arn't useful for most NLP problems\n",
    "        (token.pos_ in ['PROPN','NUM','SYM','X','PUNCT'] and not token.text==\".\") or # remove proper nouns, numbers, symbols, unknowns, and all punctuation except periods, so we can delineate sentences.\n",
    "        #token.is_oov or # Remove words that are out of vocabulary (oov). We might lose some useful data, however, to be safe we take it out as we can't identify it.\n",
    "        token.is_space # Remove blank sentences\n",
    "    )]\n",
    "#    print(post.thread_title,keep_tokens) # will print once for each post, so there will be duplicates\n",
    "\n",
    "\n",
    "# doc_strip = nlp_lg(' '.join(keep_tokens))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple, apple, apple\n",
      "bananas, banana, bananas\n",
      "cars, car, cars\n",
      "turtles, turtle, turtles\n",
      "carrot, carrot, carrot\n",
      "[('apple', 'FRUIT')]\n"
     ]
    }
   ],
   "source": [
    "for token in doc_strip:\n",
    "   print('{}, {}, {}'.format(token.text, token.lemma_, token.norm_, token.pos_))\n",
    "print([(ent.text, ent.label_) for ent in doc_strip.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9076452796594085796 NounIsAdv 0 2 Apples best\n",
      "16820485401210444574 NounIsAdj 15 17 fruits yucky\n",
      "6136520347023808161 AdjNoun 16 18 yucky vegetables\n",
      "16820485401210444574 NounIsAdj 17 19 vegetables inferior\n",
      "6136520347023808161 AdjNoun 27 29 juicy taste\n",
      "9076452796594085796 NounIsAdv 28 30 taste sooooo\n",
      "16820485401210444574 NounIsAdj 45 47 Apples gross\n",
      "6136520347023808161 AdjNoun 49 51 worst fruits\n",
      "16820485401210444574 NounIsAdj 50 52 fruits bad\n",
      "6136520347023808161 AdjNoun 66 68 filthy casual\n",
      "6136520347023808161 AdjNoun 88 90 year!. storms\n",
      "9076452796594085796 NounIsAdv 98 100 tomatoes Never\n",
      "16820485401210444574 NounIsAdj 109 111 Peaches best\n",
      "6136520347023808161 AdjNoun 110 112 best cabbage\n",
      "16820485401210444574 NounIsAdj 111 113 cabbage worst\n",
      "6136520347023808161 AdjNoun 112 114 worst end\n",
      "6136520347023808161 AdjNoun 149 151 Acidic tomatoes\n",
      "16820485401210444574 NounIsAdj 150 152 tomatoes dirty\n",
      "6136520347023808161 AdjNoun 151 153 dirty squash\n",
      "6136520347023808161 AdjNoun 154 156 dirty tomatoes\n",
      "6136520347023808161 AdjNoun 157 159 sweet fruit\n",
      "6136520347023808161 AdjNoun 162 164 yucky lettuce\n",
      "9076452796594085796 NounIsAdv 166 168 forum aka\n",
      "6136520347023808161 AdjNoun 200 202 little bitch\n",
      "6136520347023808161 AdjNoun 213 215 secret raids\n",
      "16820485401210444574 NounIsAdj 226 228 ripener entire\n",
      "6136520347023808161 AdjNoun 238 240 fuck precision\n",
      "16820485401210444574 NounIsAdj 242 244 mark fucking\n",
      "6136520347023808161 AdjNoun 243 245 fucking words\n",
      "6136520347023808161 AdjNoun 259 261 secret network\n",
      "6136520347023808161 AdjNoun 273 275 little thing\n",
      "6136520347023808161 AdjNoun 280 282 dead kid\n",
      "6136520347023808161 AdjNoun 289 291 bare hands\n",
      "6136520347023808161 AdjNoun 296 298 organic combat\n",
      "6136520347023808161 AdjNoun 300 302 entire pesticide\n",
      "6136520347023808161 AdjNoun 307 309 miserable ass\n",
      "16820485401210444574 NounIsAdj 310 312 continent little\n",
      "6136520347023808161 AdjNoun 311 313 little shit\n",
      "6136520347023808161 AdjNoun 316 318 appleholey retribution\n",
      "16820485401210444574 NounIsAdj 317 320 retribution little clever\n",
      "6136520347023808161 AdjNoun 319 321 clever comment\n",
      "6136520347023808161 AdjNoun 324 326 fucking tongue\n",
      "16820485401210444574 NounIsAdj 332 334 price goddamn\n",
      "6136520347023808161 AdjNoun 333 335 goddamn idiot\n",
      "6136520347023808161 AdjNoun 341 343 Your lettuce\n",
      "6136520347023808161 AdjNoun 345 347 dead kiddo\n"
     ]
    }
   ],
   "source": [
    "doc = doc_strip\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp_lg.vocab)\n",
    "\n",
    "noun_is_adj_plus = [{\"POS\": \"NOUN\"}, {\"POS\": \"ADV\", \"OP\": \"*\"}, {\"POS\": \"DET\", \"OP\": \"*\"},\n",
    "           {\"POS\": \"ADJ\", \"OP\": \"+\"}]\\\n",
    "\n",
    "noun_is_adj = [{\"POS\": \"NOUN\"}, {\"POS\": \"ADJ\", \"OP\": \"*\"}, {\"POS\": \"ADJ\", \"OP\": \"+\"}]\n",
    "noun_is_adv = [{\"POS\": \"NOUN\"}, {\"POS\": \"ADV\", \"OP\": \"+\"}]\n",
    "\n",
    "adj_noun = [ {\"POS\": \"ADJ\"},{\"POS\": \"NOUN\"}]\n",
    "\n",
    "matcher.add(\"NounIsAdj\", None, noun_is_adj)  # add pattern\n",
    "matcher.add(\"NounIsAdv\", None, noun_is_adv)  # add pattern\n",
    "matcher.add(\"AdjNoun\", None, adj_noun)\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "fruit_words = []\n",
    "veggie_words = []\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp_lg.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    #if not set([token.lemma_ for token in span]).isdisjoint(fruit_match_words):\n",
    "    #    print(span.text)\n",
    "    \n",
    "        \n",
    "    #if set(span.text.split(' ')) & fruit_match_words:\n",
    "    #    print(span.text)\n",
    "\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fruit mentions: 20\n",
      "Veg mentions: 15\n"
     ]
    }
   ],
   "source": [
    "doc = nlp_sm(raw)\n",
    "from collections import defaultdict\n",
    "word_count = defaultdict(int)\n",
    "for token in doc:\n",
    "    if token.lemma_ in fruit_match_words.union(vegetable_match_words):\n",
    "        word_count[token.lemma_] += 1\n",
    "\n",
    "fruit_count = sum([word_count[key] for key in word_count.keys() if key in fruit_match_words])\n",
    "veg_count = sum([word_count[key] for key in word_count.keys() if key in vegetable_match_words])\n",
    "\n",
    "print(\"Fruit mentions: {}\".format(fruit_count))\n",
    "print(\"Veg mentions: {}\".format(veg_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('apple', 'good'), ('fruit', 'yucky'), ('apple', 'gross'), ('fruit', 'which'), ('grape', 'superior'), ('peach', 'good'), ('peach', 'good')]\n"
     ]
    }
   ],
   "source": [
    "#posts = collection.find({})\n",
    "\n",
    "noun_adj_pairs = []\n",
    "\n",
    "for sent in doc.sents:\n",
    "    for i,token in enumerate(sent):\n",
    "        if token.pos_ not in ('NOUN','PROPN', 'ADJ'):\n",
    "            continue\n",
    "        for j in range(i+1,len(sent)):\n",
    "            if (token.pos_ in ('NOUN','PROPN') and sent[j].pos_ == 'ADJ'):\n",
    "                noun_adj_pairs.append((token,sent[j]))\n",
    "                break\n",
    "            if (token.pos_ == 'ADJ' and sent[j].pos_ in ('NOUN','PROPN')):\n",
    "           #    noun_adj_pairs.append((sent[j], token))\n",
    "                break\n",
    "    \n",
    "fruit_pairs = [(pair[0].lemma_,pair[1].lemma_) for pair in noun_adj_pairs if pair[0].lemma_ in fruit_match_words]\n",
    "veg_pairs = [(pair[0].lemma_,pair[1].lemma_) for pair in noun_adj_pairs if pair[0].lemma_ in vegetable_match_words]\n",
    "\n",
    "print(fruit_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "ruler = EntityRuler(nlp_lg)\n",
    "patterns = [{\"label\": \"FRUIT\", \"pattern\": [{\"lower\":\"apple\"}]},\n",
    "            {\"label\": \"FRUIT\", \"pattern\": [{\"lower\":\"grape\"}]},\n",
    "           ]\n",
    "ruler.add_patterns(patterns)\n",
    "nlp.add_pipe(ruler)\n",
    "\n",
    "threads = collection.find({\"title\":\"Apples ('pulls) are the best fruit\"})\n",
    "threads = [thread['posts'] for thread in threads]\n",
    "posts = [post for thread in threads for post in thread]\n",
    "contents = [post['content'] for post in posts]\n",
    "pprint(contents)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
