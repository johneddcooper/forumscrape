{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Automaton Forumscrape Project\n",
    "# Data Analytics\n",
    "\n",
    "This notebook explains how to derive some basic statistics from the forum post data we gathered from the [Fruits and Veggies forum](https://learningautomaton.ca/wp-content/uploads/2019/02/FruitsAndVeggiesForum/Knock%20Knock...%20-%20Fruits%20and%20Veggies.html) as part of my [Forumscrape Project](https://learningautomaton.ca/2019/01/ethical-forum-scraping-and-nlp-data-analytics-project/).\n",
    "\n",
    "In the [previous post](https://learningautomaton.ca/2019/04/forum-scrape-project-data-processing-sanitation-and-anonymizing/) we cleaned and anonymized the data we scraped from the forum.\n",
    "\n",
    "You can see a text representation of the current state of the database [here](https://github.com/johneddcooper/forumscrape/blob/master/db_refactoring/data_stg3_word_strip).\n",
    "\n",
    "\n",
    "\n",
    "The end state is that the data pulled has been cleaned, stripped of potential [personally identifying information](https://en.wikipedia.org/wiki/Personal_data), and is ready to be processed for data statistics. \n",
    "\n",
    "I delve deeper into the topic of personal information and why it is important to sanitize in the [Ethics Analasys](https://learningautomaton.ca/2019/01/forum-scrape-project-ethics/) for this project.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "For our data and needs, this will be a three-part process:\n",
    "1. Refactor the database to make it more easily accessible, and remove unnecessary fields;\n",
    "2. Replace usernames with unique random identifiers (we don't want to strip them out completely, as we can derive anonymous user statistics;\n",
    "3. Remove unnecessary, potentially identifying, and unknown words and data from post titles and content.\n",
    "\n",
    "We will iteratively write the changes to a copy of the database in each phase.\n",
    "\n",
    "## Tools\n",
    "\n",
    "For database reading we will use PyMongo, to connect to a locally running MongoDB server.  \n",
    "\n",
    "To do the languge processing and analytics, we will use [spaCy](https://spacy.io/), a free, open-source Natural Language Processing library for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "# Point the MongoClient at our database. In this case, the MongoDB server is running locally, on port 27017.\n",
    "client = MongoClient('192.168.2.70', 27017)\n",
    "# The DB within the MongoDB server the data is stored on, in my case, in a DB called fruitsandveggies\n",
    "db=client['fruitsandveggies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm') # we use spaCy's en_core_web_sm language model for demonstration, however, we would use the md or lg models normally as they are more accurate and offer more features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'posts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-2298e1311a38>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Note: if pulling the data from a database, post['thread_title'] needs to be used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mpost\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mposts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mcontent_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthread_title\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Construct a new nlp doc, which automatically tokenizes and tags, returns a spaCy Doc object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# Go through each token in the post title, only keeping the ones that don't match our stripping rules\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'posts' is not defined"
     ]
    }
   ],
   "source": [
    "# Sanitize post titles \n",
    "# Remove stop words, proper nouns, numbers, symbols, unknowns, punctuation other than periods, unknown words, and new lines.\n",
    "\n",
    "\n",
    "# If pulling from a database...\n",
    "# collection = db['aggregate-posts-out']\n",
    "# posts = collection.find({})\n",
    "# Note: if pulling the data from a database, post['thread_title'] needs to be used\n",
    "\n",
    "for post in posts:\n",
    "    content_doc = nlp(post.thread_title) # Construct a new nlp doc, which automatically tokenizes and tags, returns a spaCy Doc object \n",
    "    # Go through each token in the post title, only keeping the ones that don't match our stripping rules\n",
    "    keep_tokens = [token for token in content_doc if not( # Iterating over a spaCy Doc returns the tokens within the Doc \n",
    "        token.is_stop or # remove stop words, or words that arn't useful for most NLP problems\n",
    "        (token.pos_ in ['PROPN','NUM','SYM','X','PUNCT'] and not token.text==\".\") or # remove proper nouns, numbers, symbols, unknowns, and all punctuation except periods, so we can delineate sentences.\n",
    "        #token.is_oov or # Remove words that are out of vocabulary (oov). We might lose some useful data, however, to be safe we take it out as we can't identify it.\n",
    "        token.is_space # Remove blank sentences\n",
    "    )]\n",
    "    print(post.thread_title,keep_tokens) # will print once for each post, so there will be duplicates\n",
    "\n",
    "# Code to update the changes to a DB, run from within the for loop\n",
    "#     collection.update_many(\n",
    "#        {'thread_title':post['thread_title']},\n",
    "#        {'$set':{'thread_title':str(keep_tokens)}}\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanitize post contents\n",
    "# Remove stop words, proper nouns, numbers, symbols, unknowns, punctuation other than periods, unknown words, and new lines.\n",
    "\n",
    "# If pulling from a database...\n",
    "# collection = db['aggregate-posts-out']\n",
    "# posts = collection.find({})\n",
    "# Note: if pulling the data from a database, post['thread_title'] needs to be used\n",
    "\n",
    "for post in posts:\n",
    "    content_doc = nlp(post.content) \n",
    "    # Go through each token in the post title, only keeping the ones that don't match our stripping rules\n",
    "    keep_tokens = [token for token in content_doc if not( \n",
    "        token.is_stop or # remove stop words, or words that arn't useful for most NLP problems\n",
    "        (token.pos_ in ['PROPN','NUM','SYM','X','PUNCT'] and not token.text==\".\") or # remove proper nouns, numbers, symbols, unknowns, and all puncuation expect periods, so we can deliniate sentences.\n",
    "        #token.is_oov or # Remove words that are out of vocabulary (oov). We might lose some useful data, however to be safe we take it out as we can't identify it.\n",
    "        token.is_space # Remove blank sentances\n",
    "    )]\n",
    "    print(post.thread_title,keep_tokens)\n",
    "\n",
    "\n",
    "# Code to update the changes to a DB, run within the for loop\n",
    "#     collection.update_one(\n",
    "#        {'_id':post['_id']},\n",
    "#        {'$set':{'content':str(keep_tokens)}}\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration, lets see all of the tokens that we stripped out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped_tokens = set()\n",
    "\n",
    "for post in posts:\n",
    "    content_doc = nlp(post.content) \n",
    "    throwaway_tokens = [token for token in content_doc if  \n",
    "        token.is_stop or # remove stop words, or words that aren't useful for most NLP problems\n",
    "        (token.pos_ in ['PROPN','NUM','SYM','X','PUNCT'] and not token.text==\".\") or # remove proper nouns, numbers, symbols, unknowns, and all punctuation except periods, so we can delineate sentences.\n",
    "        token.is_oov or # Remove words that are out of vocabulary (oov). We might lose some useful data, however, to be safe we take it out as we can't identify it.\n",
    "        token.is_space # Remove blank sentences\n",
    "    ]\n",
    "    for token in throwaway_tokens:\n",
    "        stripped_tokens.add(token)\n",
    "        \n",
    "        \n",
    "print(stripped_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, we have seen:\n",
    "* Refactoring a DB using MongoDB aggregation pipelines\n",
    "* Assigning a unique number to users and replacing them in the DB\n",
    "* Stripping out unwanted words from the DB using simple language processing with spaCy\n",
    "\n",
    "In the next post, we will dive more into spaCy and NLP, and how we can derive some useful information from our cleaned dataset.\n",
    "\n",
    "If you have any comments or recommendations, please let me know by leaving a comment at https://learningautomaton.ca/forum-scrape-project---db-editing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre><code>if can_learn: \n",
    "    learn()</code></pre>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
